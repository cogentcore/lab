// Code generated by "gosl"; DO NOT EDIT
// kernel: Compute

// // Params are the parameters for the computation. 
@group(0) @binding(0)
var<storage, read_write> Params: array<ParamStruct>;
@group(0) @binding(1)
var<storage, read_write> Data: array<f32>;
@compute @workgroup_size(64, 1, 1)
fn main(@builtin(global_invocation_id) idx: vec3<u32>) {
	Compute(idx.x);
}


///////////// import: "compute.wgsl"

//gosl:import "cogentcore.org/core/math32"

//gosl:vars

// Params are the parameters for the computation.
//gosl:read-only

// Data is the data on which the computation operates.
// 2D: outer index is data, inner index is: Raw, Integ, Exp vars.
//gosl:2D

const  Raw: u32   = 0;
const  Integ: u32 = 1;
const  Exp: u32 = 2;

// ParamStruct has the test params
struct ParamStruct {

	// rate constant in msec
	Tau: f32,

	// 1/Tau
	Dt: f32,

	pad:  f32,
	pad1: f32,
}

// IntegFromRaw computes integrated value from current raw value
fn ParamStruct_IntegFromRaw(ps: ptr<function,ParamStruct>, idx: u32) {
	var integ = Data[F32Index2D(Data[0], Data[1], idx, Integ)]; // .Value(idx, Integ);
	integ += (*ps).Dt * (Data[F32Index2D(Data[0], Data[1], idx, Raw)] - integ); // .Value(idx, Raw) - integ);
	Data[F32Index2D(Data[0], Data[1], idx, Integ)] = integ; // .Set(integ, idx, Integ);
	Data[F32Index2D(Data[0], Data[1], idx, Exp)] = FastExp(-integ); // .Set(FastExp(-integ), idx, Exp);
}

// Compute does the main computation
fn Compute(i: u32) { //gosl:kernel
	var params=Params[0]; ParamStruct_IntegFromRaw(&params, i);
}


///////////// import: "fastexp.wgsl"

// FastExp is a quartic spline approximation to the Exp function, by N.N. Schraudolph
// It does not have any of the sanity checking of a standard method -- returns
// nonsense when arg is out of range.  Runs in 2.23ns vs. 6.3ns for 64bit which is faster
// than Exp actually.
fn FastExp(x: f32) -> f32 {
	if (x <= -88.02969) { // this doesn't add anything and -exp is main use-case anyway
		return f32(0.0);
	}
	var i = i32(12102203*x) + i32(127)*(i32(1)<<23);
	var m = i >> 7 & 0xFFFF; // copy mantissa
	i += (((((((((((3537 * m) >> 16) + 13668) * m) >> 18) + 15817) * m) >> 14) - 80470) * m) >> 11);
	return bitcast<f32>(u32(i));
}

// sltensor indexing functions

fn F32Index2D(s0: f32, s1: f32, i0: u32, i1: u32) -> u32 {
	return u32(2) + bitcast<u32>(s0) * i0 + bitcast<u32>(s1) * i1;
}

fn F32Index3D(s0: f32, s1: f32, s2: f32, i0: u32, i1: u32, i2: u32) -> u32 {
	return u32(3) + bitcast<u32>(s0) * i0 + bitcast<u32>(s1) * i1 + bitcast<u32>(s2) * i2;
}

fn U32Index2D(s0: u32, s1: u32, i0: u32, i1: u32) -> u32 {
	return u32(2) + s0 * i0 + s1 * i1;
}

fn U32Index3D(s0: u32, s1: u32, s2: u32, i0: u32, i1: u32, i2: u32) -> u32 {
	return u32(3) + s0 * i0 + s1 * i1 + s2 * i2;
}

