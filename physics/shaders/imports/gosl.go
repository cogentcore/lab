// Code generated by "gosl"; DO NOT EDIT

package imports

import (
	"embed"
	"fmt"
	"math"
	"unsafe"
	"cogentcore.org/core/gpu"
	"cogentcore.org/lab/tensor"
)

var shaders embed.FS

var (
	// GPUInitialized is true once the GPU system has been initialized.
	// Prevents multiple initializations.
	GPUInitialized bool
	
	// ComputeGPU is the compute gpu device.
	// Set this prior to calling GPUInit() to use an existing device.
	ComputeGPU *gpu.GPU

	// BorrowedGPU is true if our ComputeGPU is set externally,
	// versus created specifically for this system. If external,
	// we don't release it.
	BorrowedGPU bool
	
	// UseGPU indicates whether to use GPU vs. CPU.
	UseGPU bool
)
// GPUSystem is a GPU compute System with kernels operating on the
// same set of data variables.
var GPUSystem *gpu.ComputeSystem

// GPUVars is an enum for GPU variables, for specifying what to sync.
type GPUVars int32 //enums:enum

const (
	ParamsVar GPUVars = 0
	BodiesVar GPUVars = 1
	JointsVar GPUVars = 2
	JointDoFsVar GPUVars = 3
	BodyJointsVar GPUVars = 4
	BodyCollidePairsVar GPUVars = 5
	DynamicsVar GPUVars = 6
	BroadContactsNVar GPUVars = 7
	BroadContactsVar GPUVars = 8
	ContactsNVar GPUVars = 9
	ContactsVar GPUVars = 10
	JointControlsVar GPUVars = 11
)

// Tensor stride variables
var TensorStrides tensor.Uint32

// GPUInit initializes the GPU compute system,
// configuring system(s), variables and kernels.
// It is safe to call multiple times: detects if already run.
func GPUInit() {
	if GPUInitialized {
		return
	}
	GPUInitialized = true
	if ComputeGPU == nil { // set prior to this call to use an external
		ComputeGPU = gpu.NewComputeGPU()
	} else {
		BorrowedGPU = true
	}
	gp := ComputeGPU
	
	_ = fmt.Sprintf("%g",math.NaN()) // keep imports happy
	{
		sy := gpu.NewComputeSystem(gp, "Default")
		GPUSystem = sy
		vars := sy.Vars()
		{
			sgp := vars.AddGroup(gpu.Storage, "Params")
			var vr *gpu.Var
			_ = vr
			vr = sgp.Add("TensorStrides", gpu.Uint32, 1, gpu.ComputeShader)
			vr.ReadOnly = true
			vr = sgp.AddStruct("Params", int(unsafe.Sizeof(PhysParams{})), 1, gpu.ComputeShader)
			vr.ReadOnly = true
			sgp.SetNValues(1)
		}
		{
			sgp := vars.AddGroup(gpu.Storage, "Bodies")
			var vr *gpu.Var
			_ = vr
			vr = sgp.Add("Bodies", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("Joints", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("JointDoFs", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("BodyJoints", gpu.Int32, 1, gpu.ComputeShader)
			vr = sgp.Add("BodyCollidePairs", gpu.Int32, 1, gpu.ComputeShader)
			sgp.SetNValues(1)
		}
		{
			sgp := vars.AddGroup(gpu.Storage, "Dynamics")
			var vr *gpu.Var
			_ = vr
			vr = sgp.Add("Dynamics", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("BroadContactsN", gpu.Int32, 1, gpu.ComputeShader)
			vr = sgp.Add("BroadContacts", gpu.Float32, 1, gpu.ComputeShader)
			vr = sgp.Add("ContactsN", gpu.Int32, 1, gpu.ComputeShader)
			vr = sgp.Add("Contacts", gpu.Float32, 1, gpu.ComputeShader)
			sgp.SetNValues(1)
		}
		{
			sgp := vars.AddGroup(gpu.Storage, "Controls")
			var vr *gpu.Var
			_ = vr
			vr = sgp.Add("JointControls", gpu.Float32, 1, gpu.ComputeShader)
			sgp.SetNValues(1)
		}
		var pl *gpu.ComputePipeline
		pl = gpu.NewComputePipelineShaderFS(shaders, "", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl = gpu.NewComputePipelineShaderFS(shaders, "", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl = gpu.NewComputePipelineShaderFS(shaders, "", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl = gpu.NewComputePipelineShaderFS(shaders, "", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl = gpu.NewComputePipelineShaderFS(shaders, "", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl = gpu.NewComputePipelineShaderFS(shaders, "", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl = gpu.NewComputePipelineShaderFS(shaders, "", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl = gpu.NewComputePipelineShaderFS(shaders, "", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl = gpu.NewComputePipelineShaderFS(shaders, "", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl = gpu.NewComputePipelineShaderFS(shaders, "", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl = gpu.NewComputePipelineShaderFS(shaders, "", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl = gpu.NewComputePipelineShaderFS(shaders, "", sy)
		pl.AddVarUsed(0, "TensorStrides")
		pl = gpu.NewComputePipelineShaderFS(shaders, "", sy)
		pl.AddVarUsed(0, "TensorStrides")
		sy.Config()
	}
}

// GPURelease releases the GPU compute system resources.
// Call this at program exit.
func GPURelease() {
	if GPUSystem != nil {
		GPUSystem.Release()
		GPUSystem = nil
	}

	if !BorrowedGPU && ComputeGPU != nil {
		ComputeGPU.Release()
	
	}
	ComputeGPU = nil
}

// RunCollisionBroad runs the CollisionBroad kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneCollisionBroad call does Run and Done for a
// single run-and-sync case.
func RunCollisionBroad(n int) {
	if UseGPU {
		RunCollisionBroadGPU(n)
	} else {
		RunCollisionBroadCPU(n)
	}
}

// RunCollisionBroadGPU runs the CollisionBroad kernel on the GPU. See [RunCollisionBroad] for more info.
func RunCollisionBroadGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["CollisionBroad"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunCollisionBroadCPU runs the CollisionBroad kernel on the CPU.
func RunCollisionBroadCPU(n int) {
	gpu.VectorizeFunc(0, n, CollisionBroad)
}

// RunOneCollisionBroad runs the CollisionBroad kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneCollisionBroad(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunCollisionBroadGPU(n)
		RunDone(syncVars...)
	} else {
		RunCollisionBroadCPU(n)
	}
}
// RunCollisionInit runs the CollisionInit kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneCollisionInit call does Run and Done for a
// single run-and-sync case.
func RunCollisionInit(n int) {
	if UseGPU {
		RunCollisionInitGPU(n)
	} else {
		RunCollisionInitCPU(n)
	}
}

// RunCollisionInitGPU runs the CollisionInit kernel on the GPU. See [RunCollisionInit] for more info.
func RunCollisionInitGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["CollisionInit"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunCollisionInitCPU runs the CollisionInit kernel on the CPU.
func RunCollisionInitCPU(n int) {
	gpu.VectorizeFunc(0, n, CollisionInit)
}

// RunOneCollisionInit runs the CollisionInit kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneCollisionInit(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunCollisionInitGPU(n)
		RunDone(syncVars...)
	} else {
		RunCollisionInitCPU(n)
	}
}
// RunCollisionNarrow runs the CollisionNarrow kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneCollisionNarrow call does Run and Done for a
// single run-and-sync case.
func RunCollisionNarrow(n int) {
	if UseGPU {
		RunCollisionNarrowGPU(n)
	} else {
		RunCollisionNarrowCPU(n)
	}
}

// RunCollisionNarrowGPU runs the CollisionNarrow kernel on the GPU. See [RunCollisionNarrow] for more info.
func RunCollisionNarrowGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["CollisionNarrow"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunCollisionNarrowCPU runs the CollisionNarrow kernel on the CPU.
func RunCollisionNarrowCPU(n int) {
	gpu.VectorizeFunc(0, n, CollisionNarrow)
}

// RunOneCollisionNarrow runs the CollisionNarrow kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneCollisionNarrow(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunCollisionNarrowGPU(n)
		RunDone(syncVars...)
	} else {
		RunCollisionNarrowCPU(n)
	}
}
// RunDeltasFromContacts runs the DeltasFromContacts kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneDeltasFromContacts call does Run and Done for a
// single run-and-sync case.
func RunDeltasFromContacts(n int) {
	if UseGPU {
		RunDeltasFromContactsGPU(n)
	} else {
		RunDeltasFromContactsCPU(n)
	}
}

// RunDeltasFromContactsGPU runs the DeltasFromContacts kernel on the GPU. See [RunDeltasFromContacts] for more info.
func RunDeltasFromContactsGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["DeltasFromContacts"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunDeltasFromContactsCPU runs the DeltasFromContacts kernel on the CPU.
func RunDeltasFromContactsCPU(n int) {
	gpu.VectorizeFunc(0, n, DeltasFromContacts)
}

// RunOneDeltasFromContacts runs the DeltasFromContacts kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneDeltasFromContacts(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunDeltasFromContactsGPU(n)
		RunDone(syncVars...)
	} else {
		RunDeltasFromContactsCPU(n)
	}
}
// RunDeltasFromJoints runs the DeltasFromJoints kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneDeltasFromJoints call does Run and Done for a
// single run-and-sync case.
func RunDeltasFromJoints(n int) {
	if UseGPU {
		RunDeltasFromJointsGPU(n)
	} else {
		RunDeltasFromJointsCPU(n)
	}
}

// RunDeltasFromJointsGPU runs the DeltasFromJoints kernel on the GPU. See [RunDeltasFromJoints] for more info.
func RunDeltasFromJointsGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["DeltasFromJoints"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunDeltasFromJointsCPU runs the DeltasFromJoints kernel on the CPU.
func RunDeltasFromJointsCPU(n int) {
	gpu.VectorizeFunc(0, n, DeltasFromJoints)
}

// RunOneDeltasFromJoints runs the DeltasFromJoints kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneDeltasFromJoints(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunDeltasFromJointsGPU(n)
		RunDone(syncVars...)
	} else {
		RunDeltasFromJointsCPU(n)
	}
}
// RunDynamicsCurToNext runs the DynamicsCurToNext kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneDynamicsCurToNext call does Run and Done for a
// single run-and-sync case.
func RunDynamicsCurToNext(n int) {
	if UseGPU {
		RunDynamicsCurToNextGPU(n)
	} else {
		RunDynamicsCurToNextCPU(n)
	}
}

// RunDynamicsCurToNextGPU runs the DynamicsCurToNext kernel on the GPU. See [RunDynamicsCurToNext] for more info.
func RunDynamicsCurToNextGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["DynamicsCurToNext"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunDynamicsCurToNextCPU runs the DynamicsCurToNext kernel on the CPU.
func RunDynamicsCurToNextCPU(n int) {
	gpu.VectorizeFunc(0, n, DynamicsCurToNext)
}

// RunOneDynamicsCurToNext runs the DynamicsCurToNext kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneDynamicsCurToNext(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunDynamicsCurToNextGPU(n)
		RunDone(syncVars...)
	} else {
		RunDynamicsCurToNextCPU(n)
	}
}
// RunForcesFromJoints runs the ForcesFromJoints kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneForcesFromJoints call does Run and Done for a
// single run-and-sync case.
func RunForcesFromJoints(n int) {
	if UseGPU {
		RunForcesFromJointsGPU(n)
	} else {
		RunForcesFromJointsCPU(n)
	}
}

// RunForcesFromJointsGPU runs the ForcesFromJoints kernel on the GPU. See [RunForcesFromJoints] for more info.
func RunForcesFromJointsGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["ForcesFromJoints"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunForcesFromJointsCPU runs the ForcesFromJoints kernel on the CPU.
func RunForcesFromJointsCPU(n int) {
	gpu.VectorizeFunc(0, n, ForcesFromJoints)
}

// RunOneForcesFromJoints runs the ForcesFromJoints kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneForcesFromJoints(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunForcesFromJointsGPU(n)
		RunDone(syncVars...)
	} else {
		RunForcesFromJointsCPU(n)
	}
}
// RunInitDynamics runs the InitDynamics kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneInitDynamics call does Run and Done for a
// single run-and-sync case.
func RunInitDynamics(n int) {
	if UseGPU {
		RunInitDynamicsGPU(n)
	} else {
		RunInitDynamicsCPU(n)
	}
}

// RunInitDynamicsGPU runs the InitDynamics kernel on the GPU. See [RunInitDynamics] for more info.
func RunInitDynamicsGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["InitDynamics"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunInitDynamicsCPU runs the InitDynamics kernel on the CPU.
func RunInitDynamicsCPU(n int) {
	gpu.VectorizeFunc(0, n, InitDynamics)
}

// RunOneInitDynamics runs the InitDynamics kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneInitDynamics(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunInitDynamicsGPU(n)
		RunDone(syncVars...)
	} else {
		RunInitDynamicsCPU(n)
	}
}
// RunStepBodyContacts runs the StepBodyContacts kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneStepBodyContacts call does Run and Done for a
// single run-and-sync case.
func RunStepBodyContacts(n int) {
	if UseGPU {
		RunStepBodyContactsGPU(n)
	} else {
		RunStepBodyContactsCPU(n)
	}
}

// RunStepBodyContactsGPU runs the StepBodyContacts kernel on the GPU. See [RunStepBodyContacts] for more info.
func RunStepBodyContactsGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["StepBodyContacts"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunStepBodyContactsCPU runs the StepBodyContacts kernel on the CPU.
func RunStepBodyContactsCPU(n int) {
	gpu.VectorizeFunc(0, n, StepBodyContacts)
}

// RunOneStepBodyContacts runs the StepBodyContacts kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneStepBodyContacts(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunStepBodyContactsGPU(n)
		RunDone(syncVars...)
	} else {
		RunStepBodyContactsCPU(n)
	}
}
// RunStepBodyDeltas runs the StepBodyDeltas kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneStepBodyDeltas call does Run and Done for a
// single run-and-sync case.
func RunStepBodyDeltas(n int) {
	if UseGPU {
		RunStepBodyDeltasGPU(n)
	} else {
		RunStepBodyDeltasCPU(n)
	}
}

// RunStepBodyDeltasGPU runs the StepBodyDeltas kernel on the GPU. See [RunStepBodyDeltas] for more info.
func RunStepBodyDeltasGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["StepBodyDeltas"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunStepBodyDeltasCPU runs the StepBodyDeltas kernel on the CPU.
func RunStepBodyDeltasCPU(n int) {
	gpu.VectorizeFunc(0, n, StepBodyDeltas)
}

// RunOneStepBodyDeltas runs the StepBodyDeltas kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneStepBodyDeltas(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunStepBodyDeltasGPU(n)
		RunDone(syncVars...)
	} else {
		RunStepBodyDeltasCPU(n)
	}
}
// RunStepIntegrateBodies runs the StepIntegrateBodies kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneStepIntegrateBodies call does Run and Done for a
// single run-and-sync case.
func RunStepIntegrateBodies(n int) {
	if UseGPU {
		RunStepIntegrateBodiesGPU(n)
	} else {
		RunStepIntegrateBodiesCPU(n)
	}
}

// RunStepIntegrateBodiesGPU runs the StepIntegrateBodies kernel on the GPU. See [RunStepIntegrateBodies] for more info.
func RunStepIntegrateBodiesGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["StepIntegrateBodies"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunStepIntegrateBodiesCPU runs the StepIntegrateBodies kernel on the CPU.
func RunStepIntegrateBodiesCPU(n int) {
	gpu.VectorizeFunc(0, n, StepIntegrateBodies)
}

// RunOneStepIntegrateBodies runs the StepIntegrateBodies kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneStepIntegrateBodies(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunStepIntegrateBodiesGPU(n)
		RunDone(syncVars...)
	} else {
		RunStepIntegrateBodiesCPU(n)
	}
}
// RunStepJointForces runs the StepJointForces kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneStepJointForces call does Run and Done for a
// single run-and-sync case.
func RunStepJointForces(n int) {
	if UseGPU {
		RunStepJointForcesGPU(n)
	} else {
		RunStepJointForcesCPU(n)
	}
}

// RunStepJointForcesGPU runs the StepJointForces kernel on the GPU. See [RunStepJointForces] for more info.
func RunStepJointForcesGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["StepJointForces"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunStepJointForcesCPU runs the StepJointForces kernel on the CPU.
func RunStepJointForcesCPU(n int) {
	gpu.VectorizeFunc(0, n, StepJointForces)
}

// RunOneStepJointForces runs the StepJointForces kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneStepJointForces(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunStepJointForcesGPU(n)
		RunDone(syncVars...)
	} else {
		RunStepJointForcesCPU(n)
	}
}
// RunStepSolveJoints runs the StepSolveJoints kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// Can call multiple Run* kernels in a row, which are then all launched
// in the same command submission on the GPU, which is by far the most efficient.
// MUST call RunDone (with optional vars to sync) after all Run calls.
// Alternatively, a single-shot RunOneStepSolveJoints call does Run and Done for a
// single run-and-sync case.
func RunStepSolveJoints(n int) {
	if UseGPU {
		RunStepSolveJointsGPU(n)
	} else {
		RunStepSolveJointsCPU(n)
	}
}

// RunStepSolveJointsGPU runs the StepSolveJoints kernel on the GPU. See [RunStepSolveJoints] for more info.
func RunStepSolveJointsGPU(n int) {
	sy := GPUSystem
	pl := sy.ComputePipelines["StepSolveJoints"]
	ce, _ := sy.BeginComputePass()
	pl.Dispatch1D(ce, n, 64)
}

// RunStepSolveJointsCPU runs the StepSolveJoints kernel on the CPU.
func RunStepSolveJointsCPU(n int) {
	gpu.VectorizeFunc(0, n, StepSolveJoints)
}

// RunOneStepSolveJoints runs the StepSolveJoints kernel with given number of elements,
// on either the CPU or GPU depending on the UseGPU variable.
// This version then calls RunDone with the given variables to sync
// after the Run, for a single-shot Run-and-Done call. If multiple kernels
// can be run in sequence, it is much more efficient to do multiple Run*
// calls followed by a RunDone call.
func RunOneStepSolveJoints(n int, syncVars ...GPUVars) {
	if UseGPU {
		RunStepSolveJointsGPU(n)
		RunDone(syncVars...)
	} else {
		RunStepSolveJointsCPU(n)
	}
}
// RunDone must be called after Run* calls to start compute kernels.
// This actually submits the kernel jobs to the GPU, and adds commands
// to synchronize the given variables back from the GPU to the CPU.
// After this function completes, the GPU results will be available in 
// the specified variables.
func RunDone(syncVars ...GPUVars) {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	sy.ComputeEncoder.End()
	ReadFromGPU(syncVars...)
	sy.EndComputePass()
	SyncFromGPU(syncVars...)
}

// ToGPU copies given variables to the GPU for the system.
func ToGPU(vars ...GPUVars) {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case ParamsVar:
			v, _ := syVars.ValueByIndex(0, "Params", 0)
			gpu.SetValueFrom(v, Params)
		case BodiesVar:
			v, _ := syVars.ValueByIndex(1, "Bodies", 0)
			gpu.SetValueFrom(v, Bodies.Values)
		case JointsVar:
			v, _ := syVars.ValueByIndex(1, "Joints", 0)
			gpu.SetValueFrom(v, Joints.Values)
		case JointDoFsVar:
			v, _ := syVars.ValueByIndex(1, "JointDoFs", 0)
			gpu.SetValueFrom(v, JointDoFs.Values)
		case BodyJointsVar:
			v, _ := syVars.ValueByIndex(1, "BodyJoints", 0)
			gpu.SetValueFrom(v, BodyJoints.Values)
		case BodyCollidePairsVar:
			v, _ := syVars.ValueByIndex(1, "BodyCollidePairs", 0)
			gpu.SetValueFrom(v, BodyCollidePairs.Values)
		case DynamicsVar:
			v, _ := syVars.ValueByIndex(2, "Dynamics", 0)
			gpu.SetValueFrom(v, Dynamics.Values)
		case BroadContactsNVar:
			v, _ := syVars.ValueByIndex(2, "BroadContactsN", 0)
			gpu.SetValueFrom(v, BroadContactsN.Values)
		case BroadContactsVar:
			v, _ := syVars.ValueByIndex(2, "BroadContacts", 0)
			gpu.SetValueFrom(v, BroadContacts.Values)
		case ContactsNVar:
			v, _ := syVars.ValueByIndex(2, "ContactsN", 0)
			gpu.SetValueFrom(v, ContactsN.Values)
		case ContactsVar:
			v, _ := syVars.ValueByIndex(2, "Contacts", 0)
			gpu.SetValueFrom(v, Contacts.Values)
		case JointControlsVar:
			v, _ := syVars.ValueByIndex(3, "JointControls", 0)
			gpu.SetValueFrom(v, JointControls.Values)
		}
	}
}
// RunGPUSync can be called to synchronize data between CPU and GPU.
// Any prior ToGPU* calls will execute to send data to the GPU,
// and any subsequent RunDone* calls will copy data back from the GPU.
func RunGPUSync() {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	sy.BeginComputePass()
}

// ToGPUTensorStrides gets tensor strides and starts copying to the GPU.
func ToGPUTensorStrides() {
	if !UseGPU {
		return
	}
	sy := GPUSystem
	syVars := sy.Vars()
	TensorStrides.SetShapeSizes(110)
	TensorStrides.SetInt1D(Bodies.Shape().Strides[0], 0)
	TensorStrides.SetInt1D(Bodies.Shape().Strides[1], 1)
	TensorStrides.SetInt1D(Joints.Shape().Strides[0], 10)
	TensorStrides.SetInt1D(Joints.Shape().Strides[1], 11)
	TensorStrides.SetInt1D(JointDoFs.Shape().Strides[0], 20)
	TensorStrides.SetInt1D(JointDoFs.Shape().Strides[1], 21)
	TensorStrides.SetInt1D(BodyJoints.Shape().Strides[0], 30)
	TensorStrides.SetInt1D(BodyJoints.Shape().Strides[1], 31)
	TensorStrides.SetInt1D(BodyJoints.Shape().Strides[2], 32)
	TensorStrides.SetInt1D(BodyCollidePairs.Shape().Strides[0], 40)
	TensorStrides.SetInt1D(BodyCollidePairs.Shape().Strides[1], 41)
	TensorStrides.SetInt1D(Dynamics.Shape().Strides[0], 50)
	TensorStrides.SetInt1D(Dynamics.Shape().Strides[1], 51)
	TensorStrides.SetInt1D(Dynamics.Shape().Strides[2], 52)
	TensorStrides.SetInt1D(BroadContactsN.Shape().Strides[0], 60)
	TensorStrides.SetInt1D(BroadContacts.Shape().Strides[0], 70)
	TensorStrides.SetInt1D(BroadContacts.Shape().Strides[1], 71)
	TensorStrides.SetInt1D(ContactsN.Shape().Strides[0], 80)
	TensorStrides.SetInt1D(Contacts.Shape().Strides[0], 90)
	TensorStrides.SetInt1D(Contacts.Shape().Strides[1], 91)
	TensorStrides.SetInt1D(JointControls.Shape().Strides[0], 100)
	TensorStrides.SetInt1D(JointControls.Shape().Strides[1], 101)
	v, _ := syVars.ValueByIndex(0, "TensorStrides", 0)
	gpu.SetValueFrom(v, TensorStrides.Values)
}

// ReadFromGPU starts the process of copying vars to the GPU.
func ReadFromGPU(vars ...GPUVars) {
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case ParamsVar:
			v, _ := syVars.ValueByIndex(0, "Params", 0)
			v.GPUToRead(sy.CommandEncoder)
		case BodiesVar:
			v, _ := syVars.ValueByIndex(1, "Bodies", 0)
			v.GPUToRead(sy.CommandEncoder)
		case JointsVar:
			v, _ := syVars.ValueByIndex(1, "Joints", 0)
			v.GPUToRead(sy.CommandEncoder)
		case JointDoFsVar:
			v, _ := syVars.ValueByIndex(1, "JointDoFs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case BodyJointsVar:
			v, _ := syVars.ValueByIndex(1, "BodyJoints", 0)
			v.GPUToRead(sy.CommandEncoder)
		case BodyCollidePairsVar:
			v, _ := syVars.ValueByIndex(1, "BodyCollidePairs", 0)
			v.GPUToRead(sy.CommandEncoder)
		case DynamicsVar:
			v, _ := syVars.ValueByIndex(2, "Dynamics", 0)
			v.GPUToRead(sy.CommandEncoder)
		case BroadContactsNVar:
			v, _ := syVars.ValueByIndex(2, "BroadContactsN", 0)
			v.GPUToRead(sy.CommandEncoder)
		case BroadContactsVar:
			v, _ := syVars.ValueByIndex(2, "BroadContacts", 0)
			v.GPUToRead(sy.CommandEncoder)
		case ContactsNVar:
			v, _ := syVars.ValueByIndex(2, "ContactsN", 0)
			v.GPUToRead(sy.CommandEncoder)
		case ContactsVar:
			v, _ := syVars.ValueByIndex(2, "Contacts", 0)
			v.GPUToRead(sy.CommandEncoder)
		case JointControlsVar:
			v, _ := syVars.ValueByIndex(3, "JointControls", 0)
			v.GPUToRead(sy.CommandEncoder)
		}
	}
}

// SyncFromGPU synchronizes vars from the GPU to the actual variable.
func SyncFromGPU(vars ...GPUVars) {
	sy := GPUSystem
	syVars := sy.Vars()
	for _, vr := range vars {
		switch vr {
		case ParamsVar:
			v, _ := syVars.ValueByIndex(0, "Params", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Params)
		case BodiesVar:
			v, _ := syVars.ValueByIndex(1, "Bodies", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Bodies.Values)
		case JointsVar:
			v, _ := syVars.ValueByIndex(1, "Joints", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Joints.Values)
		case JointDoFsVar:
			v, _ := syVars.ValueByIndex(1, "JointDoFs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, JointDoFs.Values)
		case BodyJointsVar:
			v, _ := syVars.ValueByIndex(1, "BodyJoints", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, BodyJoints.Values)
		case BodyCollidePairsVar:
			v, _ := syVars.ValueByIndex(1, "BodyCollidePairs", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, BodyCollidePairs.Values)
		case DynamicsVar:
			v, _ := syVars.ValueByIndex(2, "Dynamics", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Dynamics.Values)
		case BroadContactsNVar:
			v, _ := syVars.ValueByIndex(2, "BroadContactsN", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, BroadContactsN.Values)
		case BroadContactsVar:
			v, _ := syVars.ValueByIndex(2, "BroadContacts", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, BroadContacts.Values)
		case ContactsNVar:
			v, _ := syVars.ValueByIndex(2, "ContactsN", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, ContactsN.Values)
		case ContactsVar:
			v, _ := syVars.ValueByIndex(2, "Contacts", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, Contacts.Values)
		case JointControlsVar:
			v, _ := syVars.ValueByIndex(3, "JointControls", 0)
			v.ReadSync()
			gpu.ReadToBytes(v, JointControls.Values)
		}
	}
}

// GetParams returns a pointer to the given global variable: 
// [Params] []PhysParams at given index. This directly processed in the GPU code,
// so this function call is an equivalent for the CPU.
func GetParams(idx uint32) *PhysParams {
	return &Params[idx]
}
